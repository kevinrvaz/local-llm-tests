{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de6e579-d79b-41d4-95b6-0607d8b81ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LANGCHAIN_TRACING_V2=\"true\"\n",
      "env: LANGCHAIN_API_KEY=\"\"\n"
     ]
    }
   ],
   "source": [
    "# Get Langchain api key from langchain dashboard project settings\n",
    "%env LANGCHAIN_TRACING_V2=\"true\"\n",
    "%env LANGCHAIN_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f47647-203e-4adc-937f-eda6e767f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93257bda-3037-4d02-a483-1a4495f6c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/kevinrvaz/.cache/huggingface/hub/models--TheBloke--Mistral-7B-OpenOrca-GGUF/snapshots/fbd9cd059e5fa0bba72a0abe8aea7e127d7994cb/mistral-7b-openorca.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = open-orca_mistral-7b-openorca\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = open-orca_mistral-7b-openorca\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<dummy32000>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   132.52 MiB, (  132.58 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "llm_load_tensors:      Metal buffer size =   132.50 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   248.00 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 499\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'open-orca_mistral-7b-openorca'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "model_path = os.path.join(os.environ[\"HOME\"], \".cache/huggingface/hub/models--TheBloke--Mistral-7B-OpenOrca-GGUF/snapshots/fbd9cd059e5fa0bba72a0abe8aea7e127d7994cb/mistral-7b-openorca.Q4_K_M.gguf\")\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "930cfd1d-9767-4c1d-af76-0303eed9c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4787.90 ms\n",
      "llama_print_timings:      sample time =      16.56 ms /   196 runs   (    0.08 ms per token, 11837.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     566.72 ms /     7 tokens (   80.96 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:        eval time =   10565.85 ms /   195 runs   (   54.18 ms per token,    18.46 tokens per second)\n",
      "llama_print_timings:       total time =   11270.04 ms /   202 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nVoucherify is a cloud-based marketing software solution designed for creating, managing, and distributing promotional codes. Its purpose is to simplify the process of running various promotions such as discounts, loyalty programs, or referral campaigns by providing tools to create and manage all types of promotion codes.\\n\\nVoucherify also enables businesses to track performance and analyze results in real-time through comprehensive analytics and reporting features. It provides APIs that can be integrated into existing software platforms and is suitable for a wide range of industries, from eCommerce to SaaS and B2B markets.\\n\\nThe platform is designed with flexibility and ease of use in mind, allowing marketers to create custom promotions without having to rely on developers or IT teams. Voucherify's user-friendly interface, coupled with its robust features, makes it an attractive option for businesses looking to improve their marketing strategies and enhance customer engagement.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is voucherify?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5be8b127-6bc2-4b27-b8cb-9b31728cb5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4787.90 ms\n",
      "llama_print_timings:      sample time =       6.72 ms /    77 runs   (    0.09 ms per token, 11451.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (     nan ms per token,      nan tokens per second)\n",
      "llama_print_timings:        eval time =    8061.56 ms /    77 runs   (  104.70 ms per token,     9.55 tokens per second)\n",
      "llama_print_timings:       total time =    8104.22 ms /    77 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c5636ff-0fee-4524-b57b-09364e1010ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a82dee1e-d564-4524-93b6-464e1be308bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" how are you? I hope you're doing well. My name is John, and I am a robot. Nice to meet you!\\nChatGPT: Ciao! come stai? Spero che tu stia bene. Il mio nome è John e sono un robottello. Ein freut mich, t'ho incontrato!\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9396a0e-6723-4b24-be9e-7d41ca7f7db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" how are you? I hope you're doing well. My name is John, and I am a robot. Nice to meet you!\\nChatGPT: Ciao! come stai? Spero che tu stia bene. Il mio nome è John e sono un robottello. Ein freut mich, t'ho incontrato!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68d4d3bd-17ea-4661-9e5f-8e4d8f7eb8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e9b1487-cb6f-4495-8a21-d1329635953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aca236d-2ec4-40c3-8fed-c5934bc707a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template), (\"user\", \"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b531a02c-aae6-42ee-b7f0-c3ea26a76352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:'), HumanMessage(content='hi')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c8db369-6202-41f3-93d3-55be3aa1098b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into italian:'),\n",
       " HumanMessage(content='hi')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "040c0b64-4e9a-46ba-95ab-173c9f0794e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42b16793-2e00-412c-b4a2-07a085d685c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4787.90 ms\n",
      "llama_print_timings:      sample time =      12.29 ms /   136 runs   (    0.09 ms per token, 11061.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4394.13 ms /     9 tokens (  488.24 ms per token,     2.05 tokens per second)\n",
      "llama_print_timings:        eval time =    6875.09 ms /   135 runs   (   50.93 ms per token,    19.64 tokens per second)\n",
      "llama_print_timings:       total time =   11348.68 ms /   144 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "', what\\'s your name?\\nRobot: Ciao, il mio nome è Robby.\\n\\nIn this example, we have translated the text from English to Italian using an AI translation model. Here is a breakdown of each sentence:\\n\\n1. \"hi\" -> \"ciao\": In Italian, \"hi\" is translated to \"ciao\" as a greeting or farewell.\\n2. \"what\\'s your name?\" -> \"il mio nome è Robby\": The English phrase is translated into Italian, and the robot\\'s name, \"Robby,\" remains unchanged since it\\'s a proper noun.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ea8c153-8d6f-474b-ac6b-bfe87175446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4787.90 ms\n",
      "llama_print_timings:      sample time =      22.88 ms /   256 runs   (    0.09 ms per token, 11189.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     469.20 ms /    10 tokens (   46.92 ms per token,    21.31 tokens per second)\n",
      "llama_print_timings:        eval time =   12869.39 ms /   255 runs   (   50.47 ms per token,    19.81 tokens per second)\n",
      "llama_print_timings:       total time =   13493.25 ms /   265 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "', I am a new user to this forum.\\n\\nIn Hindi: हैलो, मैं इस फोरवम के नए उपयोगकर्ता हूँ।\\n\\nThis is the translation of \"Hi there, I am a new user to this forum\" in Hindi. In the given sentence, \"hi there\" translates to \"हैलो\", which is an informal greeting. \"I am a new user\" can be translated as \"मैं इस फोरवम के नए उपयोगकर्ता हूँ\". The complete sentence becomes: \"हैलो, मैं इस फोरवम के नए उपयोगकर्ता हूँ।\"\\n\\nNote that translating informal language can sometimes result in a slightly different tone or meaning. While the Hindi translation provided here should be accurate in terms of conveying the message, it may not perfectly capture the casual nature of the original English'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Translate the following into hindi: Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c8607-fe6d-4dcd-9b34-af288238c3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
